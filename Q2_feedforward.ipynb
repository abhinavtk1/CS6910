{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MA23M002 - ABHINAV T K\n",
        "# Question 2"
      ],
      "metadata": {
        "id": "Dww_8H39ERNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "HGsoYQVDE9zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "MfxHtn7hE-wu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Change the shape of the data to (60000, 784)\n",
        "x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "x = x/255.0\n",
        "\n",
        "# No. of unoque classes\n",
        "c = len(np.unique(y))"
      ],
      "metadata": {
        "id": "4GX1K-FDYhXu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "BhSBM74dFlJB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax function\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ],
      "metadata": {
        "id": "vFbLBg2XFxUu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters W and b\n",
        "def initialize_parameters(nn_layers):\n",
        "  parameters = {}                         # dictionary to hold weights and biases of each layer\n",
        "  for i in range(1, len(nn_layers)):\n",
        "    parameters[\"W\"+str(i)] = np.random.randn(nn_layers[i], nn_layers[i-1]) # Randomly initializing weights and biases\n",
        "    parameters[\"b\"+str(i)] = np.random.randn(nn_layers[i], 1)\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "a02yFufoG3gN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(x, c, num_hidden_layers, num_neurons):\n",
        "  inp_features = x.shape[1]\n",
        "  c = len(np.unique(y))\n",
        "  nn_layers = [inp_features] + [num_neurons]*num_hidden_layers + [c]\n",
        "  l = len(nn_layers)  # No. of neural network layers, including input and output layers\n",
        "\n",
        "  a = {}              # dictionary to hold hidden layer (pre-activation)\n",
        "  h = {}              # dictionary to hold hidden layer (activation)\n",
        "\n",
        "  # Initialize parameter:\n",
        "  parameters = initialize_parameters(nn_layers)\n",
        "  h[0] = x.T  # input layer\n",
        "  #print(h[0].shape)\n",
        "  for i in range(1, l-1):\n",
        "    W = parameters[\"W\"+str(i)]        # weights of hidden layer i\n",
        "    b = parameters[\"b\"+str(i)]        # bias of hidden layer i\n",
        "\n",
        "    a[i] = np.matmul(W,h[i-1]) + b\n",
        "\n",
        "    # activation for hidden layers\n",
        "    h[i] = sigmoid(a[i])\n",
        "\n",
        "  # output layer\n",
        "  W = parameters[\"W\"+str(l-1)]    # weights of hidden layer i\n",
        "  b = parameters[\"b\"+str(l-1)]    # bias of hidden layer i\n",
        "\n",
        "  a[l-1] = np.matmul(W,h[l-2]) + b          # activation function for output layer\n",
        "\n",
        "  y_hat = softmax(a[l-1])\n",
        "  return y_hat    # outputs a probability distribution over the 10 classes"
      ],
      "metadata": {
        "id": "dAYJARfdmsYW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = forward_propagation(x, c, 3, 64)"
      ],
      "metadata": {
        "id": "AguSeVB5Hoci"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_hat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WsqKJz0H3n7",
        "outputId": "582df8cb-0118-4c8b-8b03-0d603bf883bc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_hat[:,0])  # probability distribution over 10 classes for the first sample of dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTQS2ObWH__D",
        "outputId": "0cca5cba-157f-487c-8f72-8f78c533ccf0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.91194013e-04 2.78376072e-04 2.19972949e-07 9.85700584e-01\n",
            " 7.44206024e-06 9.03188230e-06 5.51904175e-06 1.89783154e-04\n",
            " 9.66571057e-07 1.36168828e-02]\n"
          ]
        }
      ]
    }
  ]
}
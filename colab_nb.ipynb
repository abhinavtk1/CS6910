{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Agenda\n",
        "# 1. Implement diff optimizers\n",
        "# 2. Learn working of wandb\n",
        "# 3. Adjust code for other hyperparameters\n",
        "# 4. Create sweeps and wandb"
      ],
      "metadata": {
        "id": "l8xo7CUhtaVj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MA23M002 - ABHINAV T K <br> CS6910 - Assignment 1"
      ],
      "metadata": {
        "id": "i_lW0takXFRr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ac3HqKeUWo8Z"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Change the shape of the data to (60000, 784)\n",
        "x = x.reshape(x.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "x = x/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "# Splitting the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "4GX1K-FDYhXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81a2bd7-30e7-435b-cd1e-ac55259e8f1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset analysis and data preprocessing\n",
        "\n",
        "# no. of classes\n",
        "c = len(np.unique(y_train))\n",
        "print(\"Total no. of classes = \", c)\n",
        "\n",
        "# Input features\n",
        "inp_features = x_train.shape[1]\n",
        "print(\"Number of input features = \", inp_features)\n",
        "\n",
        "# training samples size\n",
        "m = x_train.shape[0]\n",
        "print(\"Training samples = \", m)\n",
        "\n",
        "# validation samples size\n",
        "m_val = x_val.shape[0]\n",
        "print(\"Validation samples = \", m_val)\n",
        "\n",
        "# test samples size\n",
        "m_test = x_test.shape[0]\n",
        "print(\"Test samples = \", m_test)\n",
        "\n",
        "# Class names - the index of the class names corresponds to the class label\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW-fm71NZps-",
        "outputId": "68e1e2a6-3880-498e-f8e6-1a0107de4840"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of classes =  10\n",
            "Number of input features =  784\n",
            "Training samples =  54000\n",
            "Validation samples =  6000\n",
            "Test samples =  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding y\n",
        "y_train_encoded = np.eye(np.max(y_train) + 1)[y_train].T\n",
        "y_val_encoded = np.eye(np.max(y_val) + 1)[y_val].T\n",
        "y_test_encoded = np.eye(np.max(y_test) + 1)[y_test].T\n"
      ],
      "metadata": {
        "id": "M2glbIIYcDS3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Architecture"
      ],
      "metadata": {
        "id": "CBLEkfagcmXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining activation functions and their derivatives\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def softmax_grad(x):\n",
        "    return softmax(x) * (1-softmax(x))\n"
      ],
      "metadata": {
        "id": "VOh_ylS_clT2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters W and b\n",
        "\n",
        "def initialize_parameters(nn_layers):\n",
        "  '''\n",
        "  nn_layers: a list containing the number of neurons of each layer - where each layer no. is the index of the list\n",
        "  '''\n",
        "  np.random.seed(32)\n",
        "  parameters = {}                         # dictionary to hold weights and biases of each layer\n",
        "  prev_v = {}\n",
        "  for i in range(1, len(nn_layers)):\n",
        "    parameters[\"W\"+str(i)] = np.random.rand(nn_layers[i], nn_layers[i-1])*0.01\n",
        "    parameters[\"b\"+str(i)] = np.random.rand(nn_layers[i], 1)*0.01\n",
        "\n",
        "    prev_v[\"W\"+str(i)] = np.zeros((nn_layers[i], nn_layers[i-1]))\n",
        "    prev_v[\"b\"+str(i)] = np.zeros((nn_layers[i], 1))\n",
        "  return parameters, prev_v"
      ],
      "metadata": {
        "id": "3nv3ik7CclSr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(x, nn_layers, parameters, act_fn):\n",
        "\n",
        "  l = len(nn_layers)  # 5 # No. of neural network layers, including input and output layers\n",
        "\n",
        "  a = {}              # dictionary to hold hidden layer (pre-activation)\n",
        "  h = {}              # dictionary to hold hidden layer (activation)\n",
        "\n",
        "  h[0] = x.T  # input layer\n",
        "  #print(h[0].shape)\n",
        "  for i in range(1, l-1):\n",
        "    W = parameters[\"W\"+str(i)]        # weights of hidden layer i\n",
        "    b = parameters[\"b\"+str(i)]        # bias of hidden layer i\n",
        "    a[i] = np.matmul(W,h[i-1]) + b\n",
        "\n",
        "    # activation for hidden layers\n",
        "    if act_fn == 'sigmoid':\n",
        "      h[i] = sigmoid(a[i])\n",
        "    elif act_fn == 'relu':\n",
        "      h[i] = relu(a[i])\n",
        "    elif act_fn == 'tanh':\n",
        "      h[i] = tanh(a[i])\n",
        "\n",
        "  # output layer\n",
        "  W = parameters[\"W\"+str(l-1)]    # weights of hidden layer i\n",
        "  b = parameters[\"b\"+str(l-1)]    # bias of hidden layer i\n",
        "  a[l-1] = np.matmul(W,h[l-2]) + b          # activation function for output layer\n",
        "\n",
        "  y_hat = softmax(a[l-1])\n",
        "  return y_hat, h, a                # Returns y_hat, h, a"
      ],
      "metadata": {
        "id": "dAYJARfdmsYW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "def back_propagation(y_hat, y, h, a, nn_layers, parameters, act_fn, batch_size):\n",
        "  l = len(nn_layers)\n",
        "  grads = {}            # dictionary to store gradient of loss function wrt parameters and hidden layer neurons\n",
        "\n",
        "  # Computing gradient wrt output layer\n",
        "  grads[\"grada\"+str(l-1)] = y_hat - y\n",
        "\n",
        "  for i in range(l-1,0,-1):\n",
        "    grads[\"gradW\" + str(i)] = (1/batch_size)*np.dot(grads[\"grada\" + str(i)], h[i-1].T)\n",
        "    grads[\"gradb\" + str(i)] = (1/batch_size)*np.sum(grads[\"grada\" + str(i)], axis=1, keepdims=True)\n",
        "    if i>1:\n",
        "      if act_fn == 'sigmoid':\n",
        "        grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * sigmoid_grad(a[i-1])   # Computing gradients wrt hidden layers\n",
        "      elif act_fn == 'relu':\n",
        "        grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * relu_grad(a[i-1])\n",
        "      elif act_fn == 'tanh':\n",
        "        grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * tanh_grad(a[i-1])\n",
        "  return grads"
      ],
      "metadata": {
        "id": "ddZEWVMOqRUG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdjmkJQ8R_7J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update parameter based on different optimizers"
      ],
      "metadata": {
        "id": "qdIPH9_ESA7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def param_update_sgd(parameters, grads , lr):\n",
        "    l = len(parameters) // 2    # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)]\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "yXohNXrBt7qM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def param_update_momentum(parameters, grads , lr, beta, prev_v):\n",
        "  l = len(parameters) // 2     # no. of hidden layers + outer layer\n",
        "  eta = 1.0\n",
        "  for i in range(1, l + 1):\n",
        "    prev_v[\"W\"+str(i)] = beta*prev_v[\"W\"+str(i)] + eta*grads[\"gradW\" + str(i)]\n",
        "    prev_v[\"b\"+str(i)] = beta*prev_v[\"b\"+str(i)] + eta*grads[\"gradb\" + str(i)]\n",
        "\n",
        "    parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr*prev_v[\"W\"+str(i)]\n",
        "    parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr*prev_v[\"b\"+str(i)]\n",
        "\n",
        "  return parameters, prev_v"
      ],
      "metadata": {
        "id": "j0qDteAsSOHN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def param_update_nesterov(parameters, grads , lr, beta, prev_v):\n",
        "  l = len(parameters) // 2     # no. of hidden layers + outer layer\n",
        "  eta = 1.0\n",
        "  for i in range(1, l + 1):\n",
        "    prev_v[\"W\"+str(i)] = beta*prev_v[\"W\"+str(i)] + eta*grads[\"gradW\" + str(i)]\n",
        "    prev_v[\"b\"+str(i)] = beta*prev_v[\"b\"+str(i)] + eta*grads[\"gradb\" + str(i)]\n",
        "\n",
        "    parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr*prev_v[\"W\"+str(i)]\n",
        "    parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr*prev_v[\"b\"+str(i)]\n",
        "\n",
        "  return parameters, prev_v"
      ],
      "metadata": {
        "id": "ConbxBMaZp4M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute loss and accuracy"
      ],
      "metadata": {
        "id": "EmgmueRsSHFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y, y_hat, batch_size, parameters):\n",
        "  loss = (1/batch_size)*(-1.0 * np.sum(np.multiply(y, np.log(y_hat))))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "vfSpTZrB2nYq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "gRATlU28SKwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(x_train, y_train, epochs = 10, num_hidden_layers = 3, num_neurons = 128, learning_rate = 0.001, act_fn = 'sigmoid', weight_init = 'xavier',\n",
        "                optimizer = 'sgd',  batch_size = 512, wt_decay_l2 = 0):\n",
        "  '''\n",
        "  number of epochs: 5, 10\n",
        "  number of hidden layers: 3, 4, 5\n",
        "  size of every hidden layer: 32, 64, 128\n",
        "  weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "  learning rate: 1e-3, 1e-4\n",
        "  optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "  batch size: 16, 32, 64\n",
        "  weight initialisation: random, Xavier\n",
        "  activation functions: sigmoid, tanh, ReLU\n",
        "  '''\n",
        "\n",
        "  nn_layers = [inp_features] + [num_neurons]*num_hidden_layers + [c]\n",
        "  parameters, prev_v = initialize_parameters(nn_layers)\n",
        "  params_nesterov = parameters.copy()\n",
        "  l = len(parameters) // 2     # no. of hidden layers + outer layer\n",
        "  beta = 0.9      # decay rate\n",
        "  for epoch in range(epochs):\n",
        "    for i in range(0, x_train.shape[0], batch_size):\n",
        "      batch_sz = min(batch_size, x_train.shape[0] - i)\n",
        "\n",
        "      if optimizer == 'nesterov':\n",
        "        for j in range(1, l+1):\n",
        "          params_nesterov[\"W\"+str(j)] = parameters[\"W\"+str(j)] - beta*prev_v[\"W\"+str(j)]\n",
        "          params_nesterov[\"b\"+str(j)] = parameters[\"b\"+str(j)] - beta*prev_v[\"b\"+str(j)]\n",
        "        # calculating grads for look ahead\n",
        "        y_hat, h, a = forward_propagation(x_train[i:i+batch_size,:], nn_layers, params_nesterov, act_fn)\n",
        "        grads = back_propagation(y_hat, y_train[:,i:i+batch_size], h, a, nn_layers, params_nesterov, act_fn, batch_sz)\n",
        "        # parameter update for nesterove using grad calculated by look ahead\n",
        "        parameters, prev_v = param_update_momentum(parameters, grads , lr, beta, prev_v)\n",
        "\n",
        "      else:\n",
        "        y_hat, h, a = forward_propagation(x_train[i:i+batch_size,:], nn_layers, parameters, act_fn)\n",
        "        grads = back_propagation(y_hat, y_train[:,i:i+batch_size], h, a, nn_layers, parameters, act_fn, batch_sz)\n",
        "        if optimizer == 'sgd':\n",
        "          parameters = param_update_sgd(parameters, grads, lr)\n",
        "        elif optimizer == 'momentum':\n",
        "          parameters, prev_v = param_update_momentum(parameters, grads , lr, beta, prev_v)\n",
        "\n",
        "    # Mean loss for the full training set\n",
        "    y_hat, _, _ = forward_propagation(x_train, nn_layers, parameters, act_fn)\n",
        "    cost = compute_loss(y_train, y_hat, 54000, parameters)\n",
        "    y_hat = np.argmax(y_hat, axis=0)\n",
        "    accuracy = np.mean(y_hat == y_train)\n",
        "    print(\"Epochs = \", epoch, \"\\tTraining cost:\", cost, \"\\tAccuracy:\", accuracy)\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "AxNYXccvQm_6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  iter = 0\n",
        "  epochs = 10\n",
        "  lr = 0.01 # learning rate\n",
        "  batch_size = 64\n",
        "  params = train_model(x_train, y_train_encoded, epochs = 20, act_fn = 'tanh', optimizer = 'nesterov' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zIfsf5hrMwu",
        "outputId": "2c9ef4fe-a8a7-4926-bf9d-ec989eeffa17"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs =  0 \tTraining cost: 2.151706342147235 \tAccuracy: 0.0\n",
            "Epochs =  1 \tTraining cost: 1.7750193807382109 \tAccuracy: 0.0\n",
            "Epochs =  2 \tTraining cost: 1.6602813456769059 \tAccuracy: 0.0\n",
            "Epochs =  3 \tTraining cost: 1.6142142941339568 \tAccuracy: 0.0\n",
            "Epochs =  4 \tTraining cost: 1.5842740885577553 \tAccuracy: 0.0\n",
            "Epochs =  5 \tTraining cost: 1.5601537727739827 \tAccuracy: 0.0\n",
            "Epochs =  6 \tTraining cost: 1.5376102053275325 \tAccuracy: 0.3523833333333333\n",
            "Epochs =  7 \tTraining cost: 2.6548039680183444 \tAccuracy: 6.666666666666667e-05\n",
            "Epochs =  8 \tTraining cost: 2.851006063161291 \tAccuracy: 0.0\n",
            "Epochs =  9 \tTraining cost: 2.8157534858214115 \tAccuracy: 0.0\n",
            "Epochs =  10 \tTraining cost: 3.0188074111589223 \tAccuracy: 0.03635740740740741\n",
            "Epochs =  11 \tTraining cost: 2.856454948927334 \tAccuracy: 0.005518518518518518\n",
            "Epochs =  12 \tTraining cost: 2.336648585208616 \tAccuracy: 0.030562962962962962\n",
            "Epochs =  13 \tTraining cost: 2.0455115785616402 \tAccuracy: 0.0\n",
            "Epochs =  14 \tTraining cost: 1.9680194625018743 \tAccuracy: 0.028512962962962962\n",
            "Epochs =  15 \tTraining cost: 2.116329167141161 \tAccuracy: 0.050614814814814814\n",
            "Epochs =  16 \tTraining cost: 1.633828375021378 \tAccuracy: 0.029705555555555554\n",
            "Epochs =  17 \tTraining cost: 1.6464594942846191 \tAccuracy: 0.02105\n",
            "Epochs =  18 \tTraining cost: 1.4259982011082741 \tAccuracy: 0.045753703703703706\n",
            "Epochs =  19 \tTraining cost: 1.2353251782756711 \tAccuracy: 0.10088148148148147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPnZ9PlmtJBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
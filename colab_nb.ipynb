{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MA23M002 - ABHINAV T K <br> CS6910 - Assignment 1"
      ],
      "metadata": {
        "id": "i_lW0takXFRr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ac3HqKeUWo8Z"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, x, y, x_test, y_test):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.x_test = x_test\n",
        "    self.y_test = y_test\n",
        "    self.c = len(np.unique(y))        # no. of classes\n",
        "\n",
        "     # Class names - the index of the class names corresponds to the class label\n",
        "    self.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    self.hl = 0       # No. of hidden layers\n",
        "    self.nn = 0       # No. of neurons in a hidden layer\n",
        "  def preprocess(self):\n",
        "    # change shape\n",
        "    self.x = self.x.reshape(self.x.shape[0], -1)\n",
        "    self.inp_features = self.x.shape[1]    # no. of input features\n",
        "\n",
        "    self.x_test = self.x_test.reshape(self.x_test.shape[0], -1)\n",
        "    # normalize data\n",
        "    self.x = self.x/255.0\n",
        "    self.x_test = self.x_test/255.0\n",
        "\n",
        "  def split_data(self):\n",
        "    self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x, self.y, test_size=0.1,random_state=42)\n",
        "\n",
        "  def print_data_details(self):\n",
        "    print(\"Total no. of classes = \", self.c)\n",
        "    print(\"The class names are: \", self.classes)\n",
        "    print(\"Number of input features = \", self.inp_features)\n",
        "\n",
        "    # training samples size\n",
        "    self.m = self.x_train.shape[0]\n",
        "    print(\"Training samples = \", self.m)\n",
        "\n",
        "    # validation samples size\n",
        "    self.m_val = self.x_val.shape[0]\n",
        "    print(\"Validation samples = \", self.m_val)\n",
        "\n",
        "    # test samples size\n",
        "    self.m_test = self.x_test.shape[0]\n",
        "    print(\"Test samples = \", self.m_test)\n",
        "\n",
        "  def one_hot_encod(self):\n",
        "    self.y_train_encoded = np.eye(np.max(self.y_train) + 1)[self.y_train].T\n",
        "    self.y_val_encoded = np.eye(np.max(self.y_val) + 1)[self.y_val].T\n",
        "    self.y_test_encoded = np.eye(np.max(self.y_test) + 1)[self.y_test].T\n",
        "    return self.y_train_encoded, self.y_val_encoded, self.y_test_encoded\n",
        "  # Initializing parameters W and b\n",
        "  def initialize_parameters(self, nn_layers, weight_init=\"random\"):\n",
        "    '''\n",
        "    nn_layers: a list containing the number of neurons of each layer - where each layer no. is the index of the list\n",
        "    '''\n",
        "    np.random.seed(42)\n",
        "    parameters = {}                         # dictionary to hold weights and biases of each layer\n",
        "    u_t = {}\n",
        "    for i in range(1, len(nn_layers)):\n",
        "      if weight_init == \"xavier\":\n",
        "        parameters[\"W\"+str(i)] = np.random.randn(nn_layers[i],nn_layers[i-1])*np.sqrt(2/(nn_layers[i]+nn_layers[i-1]))\n",
        "        parameters[\"b\"+str(i)] = np.random.randn(nn_layers[i], 1)*np.sqrt(2/(nn_layers[i]))\n",
        "      elif weight_init == \"random\":\n",
        "        parameters[\"W\"+str(i)] = np.random.randn(nn_layers[i], nn_layers[i-1])\n",
        "        parameters[\"b\"+str(i)] = np.random.randn(nn_layers[i], 1)\n",
        "\n",
        "      u_t[\"W\"+str(i)] = np.zeros((nn_layers[i], nn_layers[i-1]))\n",
        "      u_t[\"b\"+str(i)] = np.zeros((nn_layers[i], 1))\n",
        "    return parameters, u_t\n",
        "\n",
        "  # Activation functions\n",
        "  def relu(self, x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def softmax(self, x):\n",
        "      return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "  # Derivatives of activation functions\n",
        "\n",
        "  def sigmoid_grad(self, x):\n",
        "      return self.sigmoid(x) * (1-self.sigmoid(x))\n",
        "\n",
        "  def relu_grad(self, x):\n",
        "      return 1*(x>0)\n",
        "\n",
        "  def tanh_grad(self, x):\n",
        "      return (1 - (np.tanh(x)**2))\n",
        "\n",
        "  def softmax_grad(self, x):\n",
        "      return self.softmax(x) * (1-self.softmax(x))\n",
        "\n",
        "\n",
        " # Forward propagation\n",
        "  def forward_propagation(self, x, nn_layers, parameters, act_fn):\n",
        "\n",
        "    l = len(nn_layers)  # No. of neural network layers, including input and output layers\n",
        "\n",
        "    a = {}              # dictionary to hold hidden layer (pre-activation)\n",
        "    h = {}              # dictionary to hold hidden layer (activation)\n",
        "\n",
        "    h[0] = x.T  # input layer\n",
        "\n",
        "    for i in range(1, l-1):\n",
        "      W = parameters[\"W\"+str(i)]        # weights of hidden layer i\n",
        "      b = parameters[\"b\"+str(i)]        # bias of hidden layer i\n",
        "\n",
        "      a[i] = np.matmul(W,h[i-1]) + b\n",
        "\n",
        "      # activation for hidden layers\n",
        "      if act_fn == 'sigmoid':\n",
        "        h[i] = self.sigmoid(a[i])\n",
        "      elif act_fn == 'relu':\n",
        "        h[i] = self.relu(a[i])\n",
        "      elif act_fn == 'tanh':\n",
        "        h[i] = self.tanh(a[i])\n",
        "\n",
        "    # output layer\n",
        "    W = parameters[\"W\"+str(l-1)]              # weights of hidden layer i\n",
        "    b = parameters[\"b\"+str(l-1)]              # bias of hidden layer i\n",
        "\n",
        "    a[l-1] = np.matmul(W,h[l-2]) + b          # activation function for output layer\n",
        "\n",
        "    y_hat = self.softmax(a[l-1])\n",
        "    return y_hat, h, a                # Returns y_hat, h, a\n",
        "\n",
        "  # Backpropagation\n",
        "  def back_propagation(self, y_hat, y, h, a, nn_layers, parameters, act_fn, batch_size, l2_reg):\n",
        "    l = len(nn_layers)\n",
        "    grads = {}            # dictionary to store gradient of loss function wrt parameters and hidden layer neurons\n",
        "\n",
        "    # Computing gradient wrt output layer\n",
        "    grads[\"grada\"+str(l-1)] = y_hat - y\n",
        "\n",
        "    for i in range(l-1,0,-1):\n",
        "      grads[\"gradW\" + str(i)] = (1/batch_size) * (np.dot(grads[\"grada\" + str(i)], h[i-1].T) + l2_reg*parameters[\"W\"+str(i)])\n",
        "      grads[\"gradb\" + str(i)] = (1/batch_size) * np.sum(grads[\"grada\" + str(i)], axis=1, keepdims=True)\n",
        "      if i>1:\n",
        "        if act_fn == 'sigmoid':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.sigmoid_grad(a[i-1])   # Computing gradients wrt hidden layers\n",
        "        elif act_fn == 'relu':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.relu_grad(a[i-1])\n",
        "        elif act_fn == 'tanh':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.tanh_grad(a[i-1])\n",
        "    return grads\n",
        "\n",
        "  # Update parameter based on different optimizers\n",
        "  def sgd(self, parameters, grads , lr):\n",
        "    l = self.hl + 1    # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)]\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)]\n",
        "    return parameters\n",
        "\n",
        "  def momentum(self, parameters, grads , lr, beta, u_t):\n",
        "    # u_t: accumulating history of the gradient of the parameters\n",
        "\n",
        "    l = self.hl + 1     # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "      u_t[\"W\"+str(i)] = beta*u_t[\"W\"+str(i)] + lr*grads[\"gradW\" + str(i)]\n",
        "      u_t[\"b\"+str(i)] = beta*u_t[\"b\"+str(i)] + lr*grads[\"gradb\" + str(i)]\n",
        "\n",
        "      parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - u_t[\"W\"+str(i)]\n",
        "      parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - u_t[\"b\"+str(i)]\n",
        "\n",
        "    return parameters, u_t\n",
        "\n",
        "  def nesterov(self, parameters, grads , lr, beta, u_t):\n",
        "    # u_t: accumulating history of the gradient of the parameters\n",
        "\n",
        "    l = self.hl + 1     # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "      u_t[\"W\"+str(i)] = beta*u_t[\"W\"+str(i)] + lr*grads[\"gradW\" + str(i)]\n",
        "      u_t[\"b\"+str(i)] = beta*u_t[\"b\"+str(i)] + lr*grads[\"gradb\" + str(i)]\n",
        "\n",
        "      parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - u_t[\"W\"+str(i)]\n",
        "      parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - u_t[\"b\"+str(i)]\n",
        "    return parameters, u_t\n",
        "\n",
        "  def rmsprop(self, parameters, grads, lr, beta, v_t):\n",
        "    # adapts learning rate of each parameter based on magnitude of recent gradients\n",
        "    # v_t: history of the gradient of parameters\n",
        "\n",
        "    l = self.hl + 1         # no. of hidden layers + outer layer\n",
        "    eps = 1e-4\n",
        "    for i in range(1, l + 1):\n",
        "\n",
        "        v_t[\"W\"+str(i)] = beta*v_t[\"W\" + str(i)] + (1-beta)*np.square(grads[\"gradW\" + str(i)])\n",
        "        v_t[\"b\"+str(i)] = beta*v_t[\"b\" + str(i)] + (1-beta)*np.square(grads[\"gradb\" + str(i)])\n",
        "\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)] / (np.sqrt(v_t[\"W\"+str(i)] + eps))\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)] / (np.sqrt(v_t[\"b\"+str(i)] + eps))\n",
        "    return parameters, v_t\n",
        "\n",
        "  def adam(self, parameters, grads, lr, m_w, v_w, step):\n",
        "    l = self.hl + 1       # number of layers in the neural network\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    eps = 1e-8\n",
        "\n",
        "    for i in range(1, l+1):\n",
        "        # computing intermediate values\n",
        "        m_w[\"W\"+str(i)] = beta1*m_w[\"W\"+str(i)] + (1-beta1)*grads[\"gradW\"+str(i)]\n",
        "        m_w[\"b\"+str(i)] = beta1*m_w[\"b\"+str(i)] + (1-beta1)*grads[\"gradb\"+str(i)]\n",
        "        v_w[\"W\"+str(i)] = beta2*v_w[\"W\"+str(i)] + (1-beta2)*np.square(grads[\"gradW\"+str(i)])\n",
        "        v_w[\"b\"+str(i)] = beta2*v_w[\"b\"+str(i)] + (1-beta2)*np.square(grads[\"gradb\"+str(i)])\n",
        "\n",
        "        m_w_hat = m_w[\"W\"+str(i)]/(1.0 - beta1**step)\n",
        "        m_b_hat = m_w[\"b\"+str(i)]/(1.0 - beta1**step)\n",
        "        v_w_hat = v_w[\"W\"+str(i)]/(1.0 - beta2**step)\n",
        "        v_b_hat = v_w[\"b\"+str(i)]/(1.0 - beta2**step)\n",
        "\n",
        "        #update parameters\n",
        "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - (lr * m_w_hat)/np.sqrt(v_w_hat + eps)\n",
        "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - (lr * m_b_hat)/np.sqrt(v_b_hat + eps)\n",
        "\n",
        "    step = step + 1\n",
        "    return parameters, m_w, v_w, step\n",
        "\n",
        "  def nadam(self, parameters, grads, lr, m_w, v_w, step):\n",
        "\n",
        "    l = self.hl + 1     # number of layers in the neural network\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    eps = 1e-8\n",
        "\n",
        "    for i in range(1, l+1):\n",
        "        # computing intermediate values\n",
        "        m_w[\"W\"+str(i)] = beta1*m_w[\"W\"+str(i)] + (1-beta1)*grads[\"gradW\"+str(i)]\n",
        "        m_w[\"b\"+str(i)] = beta1*m_w[\"b\"+str(i)] + (1-beta1)*grads[\"gradb\"+str(i)]\n",
        "        v_w[\"W\"+str(i)] = beta2*v_w[\"W\"+str(i)] + (1-beta2)*np.square(grads[\"gradW\"+str(i)])\n",
        "        v_w[\"b\"+str(i)] = beta2*v_w[\"b\"+str(i)] + (1-beta2)*np.square(grads[\"gradb\"+str(i)])\n",
        "\n",
        "        m_w_hat = m_w[\"W\"+str(i)]/(1.0 - beta1**step)\n",
        "        m_b_hat = m_w[\"b\"+str(i)]/(1.0 - beta1**step)\n",
        "        v_w_hat = v_w[\"W\"+str(i)]/(1.0 - beta2**step)\n",
        "        v_b_hat = v_w[\"b\"+str(i)]/(1.0 - beta2**step)\n",
        "\n",
        "        #update parameters\n",
        "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - (lr /np.sqrt(v_w_hat + eps))*(beta1*m_w_hat +\n",
        "                                                                                        (1-beta1)*grads[\"gradW\"+str(i)]/(1.0 - beta1**step))\n",
        "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - (lr /np.sqrt(v_b_hat + eps))*(beta1*m_b_hat +\n",
        "                                                                                        (1-beta1)*grads[\"gradb\"+str(i)]/(1.0 - beta1**step))\n",
        "\n",
        "    step = step + 1\n",
        "    return parameters, m_w, v_w, step\n",
        "\n",
        "  def compute_loss(self, y, y_hat, batch_size, parameters, l2_reg):\n",
        "\n",
        "    wt_norm_sq = 0\n",
        "    for i in range(1, self.hl+2):\n",
        "        wt_norm_sq = wt_norm_sq + np.sum(np.square(parameters[\"W\"+str(i)]))\n",
        "\n",
        "    loss_val = (-1/batch_size)*(np.sum(np.multiply(y, np.log(y_hat))) + 0.5*l2_reg*wt_norm_sq)\n",
        "    return loss_val\n",
        "\n",
        "  # Plotting function\n",
        "  def plot_loss_curve(self, training_loss, validation_loss):\n",
        "      \"\"\"\n",
        "      Plot the training and validation loss curves.\n",
        "      \"\"\"\n",
        "      n1 = len(training_loss)\n",
        "      n2 = len(validation_loss)\n",
        "      plt.plot(list(range(n1)), training_loss, 'b', label=\"Training Loss\")\n",
        "      plt.plot(list(range(n2)), validation_loss, 'r', label=\"Validation Loss\")\n",
        "\n",
        "      # Adding title and labels\n",
        "      plt.title(\"Training and Validation Loss vs Epochs\")\n",
        "      plt.xlabel(\"# Epochs\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "\n",
        "      plt.grid()\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  def train_model(self, epochs = 10, num_hidden_layers = 3, num_neurons = 128, lr = 0.001, act_fn = 'relu',\n",
        "                    weight_init = 'xavier', optimizer = 'momentum',  batch_size = 512, l2_reg = 0.0005):\n",
        "\n",
        "    '''\n",
        "    number of epochs: 5, 10\n",
        "    number of hidden layers: 3, 4, 5\n",
        "    size of every hidden layer: 32, 64, 128\n",
        "    weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "    learning rate: 1e-3, 1e-4\n",
        "    optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "    batch size: 16, 32, 64\n",
        "    weight initialisation: random, xavier\n",
        "    activation functions: sigmoid, tanh, relu\n",
        "    '''\n",
        "\n",
        "    self.hl = num_hidden_layers\n",
        "    self.nn = num_neurons\n",
        "    nn_layers = [self.inp_features] + [self.nn]*self.hl + [self.c]\n",
        "    parameters, u_t = self.initialize_parameters(nn_layers, weight_init)\n",
        "\n",
        "    params_lookahead = parameters.copy()                 # look ahead parameters init for nesterov/nadam\n",
        "\n",
        "    # adam/nadam optimizer utils\n",
        "    m_w = u_t.copy()\n",
        "    v_w = u_t.copy()\n",
        "    step = 1\n",
        "\n",
        "    l = num_hidden_layers + 1                                # no. of hidden layers + outer layer\n",
        "    beta = 0.9                                                # decay rate\n",
        "\n",
        "\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    if optimizer == 'sgd':\n",
        "        batch_size = 1\n",
        "    for epoch in range(1, epochs+1):\n",
        "      for i in range(0, self.x_train.shape[0], batch_size):\n",
        "        batch_sz = min(batch_size, self.x_train.shape[0] - i)\n",
        "        y_hat, h, a = self.forward_propagation(self.x_train[i:i+batch_sz,:], nn_layers, parameters, act_fn)\n",
        "        if optimizer == 'nesterov' or optimizer == 'nadam':\n",
        "          # calculating look-ahead parameters\n",
        "          for j in range(1, l+1):\n",
        "            params_lookahead[\"W\"+str(j)] = parameters[\"W\"+str(j)] - beta*u_t[\"W\"+str(j)]\n",
        "            params_lookahead[\"b\"+str(j)] = parameters[\"b\"+str(j)] - beta*u_t[\"b\"+str(j)]\n",
        "          # calculating gradients of look-ahead\n",
        "          grads = self.back_propagation(y_hat, self.y_train_encoded[:,i:i+batch_sz], h, a, nn_layers, params_lookahead, act_fn, batch_sz, l2_reg)\n",
        "          if optimizer == 'nesterov':\n",
        "            parameters, u_t = self.nesterov(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'nadam':\n",
        "            parameters, m_w, v_w, step = self.nadam(parameters, grads, lr, m_w, v_w, step)\n",
        "\n",
        "        else:\n",
        "          grads = self.back_propagation(y_hat, self.y_train_encoded[:,i:i+batch_sz], h, a, nn_layers, parameters, act_fn, batch_sz, l2_reg)\n",
        "          if optimizer == 'sgd':\n",
        "            parameters = self.sgd(parameters, grads, lr)\n",
        "          elif optimizer == 'momentum':\n",
        "            parameters, u_t = self.momentum(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'rmsprop':\n",
        "            parameters, u_t = self.rmsprop(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'adam':\n",
        "            parameters, m_w, v_w, step = self.adam(parameters, grads, lr, m_w, v_w, step)\n",
        "\n",
        "      # model evaluation\n",
        "      y_hat, _, _ = self.forward_propagation(self.x_train, nn_layers, parameters, act_fn)\n",
        "      cost = self.compute_loss(self.y_train_encoded, y_hat, y_hat.shape[1], parameters, l2_reg)\n",
        "      training_loss.append(cost)\n",
        "\n",
        "      #one-hot encoding y_hat\n",
        "      y_hat_encoded = np.zeros_like(y_hat)\n",
        "      for i in range(y_hat.shape[1]):\n",
        "        max_index = np.argmax(y_hat[:, i])\n",
        "        y_hat_encoded[max_index, i] = 1\n",
        "      accuracy = np.mean(y_hat_encoded == self.y_train_encoded)\n",
        "\n",
        "      # loss for the validation set\n",
        "      y_val_hat, _, _ = self.forward_propagation(self.x_val, nn_layers, parameters, act_fn)\n",
        "      val_cost = self.compute_loss(self.y_val_encoded, y_val_hat, y_val_hat.shape[1], parameters, l2_reg)\n",
        "      validation_loss.append(val_cost)\n",
        "      #one-hot encoding y_val_hat\n",
        "      y_val_hat_encoded = np.zeros_like(y_val_hat)\n",
        "      for i in range(y_val_hat.shape[1]):\n",
        "        max_index = np.argmax(y_val_hat[:, i])\n",
        "        y_val_hat_encoded[max_index, i] = 1\n",
        "      val_accuracy = np.mean(y_val_hat_encoded == self.y_val_encoded)\n",
        "      print(\"Epochs = \", epoch, \"\\tTraining cost:\", cost, \"\\tAccuracy:\", accuracy, \"\\t Validation Accuracy:\", val_accuracy)\n",
        "      # Export data to wandb\n",
        "      #wandb.log({ 'epoch': epoch, 'loss': cost, 'accuracy': accuracy * 100})\n",
        "      #wandb.log({ 'epoch': epoch, 'validation_loss': val_cost, 'validation_accuracy': val_accuracy * 100})\n",
        "\n",
        "    self.plot_loss_curve(training_loss, validation_loss)\n",
        "    return parameters\n",
        "\n",
        "  def test_model(self, parameters, num_hidden_layers, num_neurons, act_fn, l2_reg):\n",
        "    self.hl = num_hidden_layers\n",
        "    self.nn = num_neurons\n",
        "    nn_layers = [self.inp_features] + [self.nn]*self.hl + [self.c]\n",
        "    y_hat, _, _ = self.forward_propagation(self.x_test, nn_layers, parameters, act_fn)\n",
        "    cost = self.compute_loss(self.y_test_encoded, y_hat, y_hat.shape[1], parameters, l2_reg)\n",
        "    #one-hot encoding y_hat\n",
        "    y_hat_encoded = np.zeros_like(y_hat)\n",
        "    for i in range(y_hat.shape[1]):\n",
        "      max_index = np.argmax(y_hat[:, i])\n",
        "      y_hat_encoded[max_index, i] = 1\n",
        "    accuracy = np.mean(y_hat_encoded == self.y_test_encoded)\n",
        "    print(\"Test Cost:\", cost, \"\\t Test Accuracy:\", accuracy)\n",
        "    #confused_mat = confusion_matrix(np.argmax(self.y_test_encoded, axis=1), np.argmax(y_hat_encoded, axis=1))\n",
        "    #print(\"Confusion Matrix:\")\n",
        "    #print(confused_mat)\n",
        "    #img  = plt.matshow(confused_mat)\n",
        "    #plt.title('Confusion matrix plot')\n",
        "    #plt.colorbar()\n",
        "    #plt.show()\n",
        "    return y_hat_encoded, cost, accuracy"
      ],
      "metadata": {
        "id": "F_Vclaf4VCTR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using wandb for hyperparameter tuning"
      ],
      "metadata": {
        "id": "myJr_nVfSEH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "rFH9Z3EsSIsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc715401-e873-4582-8b1e-158f000e791a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import random"
      ],
      "metadata": {
        "id": "55S316Udot0O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs = 10, num_hidden_layers = 1, num_neurons = 128, learning_rate = 0.001, act_fn = 'sigmoid', weight_init = 'xavier',\n",
        "                optimizer = 'sgd',  batch_size = 512, wt_decay_l2 = 0 <br>\n",
        "x_train, y_train, epochs = 10, num_hidden_layers = 3, num_neurons = 128, learning_rate = 0.001, act_fn = 'tanh', weight_init = 'xavier',\n",
        "                optimizer = 'sgd',  batch_size = 512, wt_decay_l2 = 0"
      ],
      "metadata": {
        "id": "By4s9mWrpE4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config file       # https://docs.wandb.ai/guides/sweeps/walkthrough\n",
        "sweep_config = {\n",
        "    'method': 'bayes',     # grid, random, bayes\n",
        "    'name' : 'sweep - fashion mnist 100 runs',\n",
        "\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'validation_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'num_hidden_layers':{\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "         'num_neurons':{\n",
        "            'values':[32, 64, 128]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            'values':[1e-3, 1e-4]\n",
        "        },\n",
        "        'act_fn': {\n",
        "            'values': ['sigmoid', 'tanh', 'relu']\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'l2_reg':{\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, project='MA23M002_A1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "SqDMRmgmo5RZ",
        "outputId": "ec70c69c-b72b-45c3-a82b-6d99bf788ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: wcdalawb\n",
            "Sweep URL: https://wandb.ai/abhinavtk/MA23M002_A1/sweeps/wcdalawb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  '''\n",
        "  WandB calls main function each time with differnet combination.\n",
        "\n",
        "  We can retrive the same and use the same values for our hypermeters.\n",
        "\n",
        "  '''\n",
        "\n",
        "  with wandb.init() as run:\n",
        "    run_name= \"-ep_\"+str(wandb.config.epochs) +\"-ac-fn_\"+ wandb.config.act_fn + \"-lr_\" + str(wandb.config.learning_rate) \\\n",
        "    +\"-opt_\"+wandb.config.optimizer+\"-bs_\"+str(wandb.config.batch_size) +\"-wt_init\"+wandb.config.weight_init\\\n",
        "    +\"-hl_\"+str(wandb.config.num_hidden_layers)+\"-hs_\"+str(wandb.config.num_neurons)\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "    # Load the Fashion MNIST dataset\n",
        "    (x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "    nn.preprocess()\n",
        "    nn.split_data()\n",
        "    nn.print_data_details()\n",
        "    nn.one_hot_encod()\n",
        "    # hyperparameter values obtained from wandb config\n",
        "    parameters = nn.train_model(epochs = wandb.config.epochs,\n",
        "                             num_hidden_layers = wandb.config.num_hidden_layers, num_neurons = wandb.config.num_neurons,\n",
        "                             lr = wandb.config.learning_rate, act_fn = wandb.config.act_fn, weight_init = wandb.config.weight_init,\n",
        "                             optimizer = wandb.config.optimizer,  batch_size = wandb.config.batch_size, l2_reg = wandb.config.l2_reg)\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 100) # calls main function for count number of times\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "S28bVLcnqEQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  # Load the Fashion MNIST dataset\n",
        "  (x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "  nn.preprocess()\n",
        "  nn.split_data()\n",
        "  nn.print_data_details()\n",
        "  nn.one_hot_encod()\n",
        "  parameters = nn.train_model(epochs=5, optimizer='nesterov',  batch_size = 512)"
      ],
      "metadata": {
        "id": "1zIfsf5hrMwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting test results and plotting confusion matrix in wandb"
      ],
      "metadata": {
        "id": "3QduISJjIBGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config file       # https://docs.wandb.ai/guides/sweeps/walkthrough\n",
        "sweep_config = {\n",
        "    'method': 'grid',     # grid, random, bayes\n",
        "    'name' : 'sweep assignment 1 - confusion matrix',\n",
        "    'parameters': {\n",
        "\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        }\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, project='MA23M002-Assignment1')"
      ],
      "metadata": {
        "id": "T7orWyhnKmLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303aad34-54cb-45bc-b7c1-35f7e166ce6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 9ly5neh8\n",
            "Sweep URL: https://wandb.ai/abhinavtk/MA23M002-Assignment1/sweeps/9ly5neh8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run test set on best model\n",
        "def main():\n",
        "  # Load the Fashion MNIST dataset\n",
        "  with wandb.init() as run:\n",
        "    (x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "    nn.preprocess()\n",
        "    nn.split_data()\n",
        "    nn.print_data_details()\n",
        "    _, _, y_test = nn.one_hot_encod()\n",
        "\n",
        "    # Best hyperparameters after tuning with wandb\n",
        "    epochs = 10\n",
        "    num_hidden_layers = 3\n",
        "    num_neurons = 128\n",
        "    learning_rate = 1e-3\n",
        "    act_fn = 'relu'\n",
        "    optimizer = 'adam'\n",
        "    batch_size = 128\n",
        "    weight_init = 'xavier'\n",
        "    l2_reg = 0.0005\n",
        "    parameters = nn.train_model(epochs, num_hidden_layers, num_neurons,\n",
        "                                learning_rate, act_fn, weight_init,\n",
        "                                optimizer, batch_size, l2_reg)\n",
        "    y_hat, _, _ = nn.test_model(parameters, num_hidden_layers, num_neurons, act_fn, l2_reg)\n",
        "    # create confusion matrix in wandb  #https://wandb.ai/wandb/plots/reports/Confusion-Matrix-Usage-and-Examples--VmlldzozMDg1NTM\n",
        "    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    ground_truth_class_ids = y_test.T.argmax(axis=1)\n",
        "    top_pred_ids = y_hat.T.argmax(axis=1)\n",
        "    wandb.log({\"confusion_matrix\" : wandb.plot.confusion_matrix(\n",
        "                preds = top_pred_ids, y_true = ground_truth_class_ids,\n",
        "                class_names= classes )})\n",
        "wandb.agent(sweep_id, function = main, count = 1)   # calls main function for count number of times\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Q9LfVNZGrNCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjit5nTu7vxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
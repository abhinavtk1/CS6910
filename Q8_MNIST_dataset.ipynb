{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwwvj2ufjf7iyClIo6MFKa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MA23M002 - ABHINAV T K <br> CS6910 - Assignment 1"
      ],
      "metadata": {
        "id": "i_lW0takXFRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q"
      ],
      "metadata": {
        "id": "SIAOlvr7Mtrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ac3HqKeUWo8Z"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "from types import SimpleNamespace\n",
        "import random\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, x, y, x_test, y_test):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.x_test = x_test\n",
        "    self.y_test = y_test\n",
        "    self.c = len(np.unique(y))        # no. of classes\n",
        "\n",
        "     # Class names - the index of the class names corresponds to the class label\n",
        "    self.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    self.hl = 0       # No. of hidden layers\n",
        "    self.nn = 0       # No. of neurons in a hidden layer\n",
        "  def preprocess(self):\n",
        "    # change shape\n",
        "    self.x = self.x.reshape(self.x.shape[0], -1)\n",
        "    self.inp_features = self.x.shape[1]    # no. of input features\n",
        "\n",
        "    self.x_test = self.x_test.reshape(self.x_test.shape[0], -1)\n",
        "    # normalize data\n",
        "    self.x = self.x/255.0\n",
        "    self.x_test = self.x_test/255.0\n",
        "\n",
        "  def split_data(self):\n",
        "    self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x, self.y, test_size=0.1,random_state=42)\n",
        "\n",
        "  def print_data_details(self):\n",
        "    print(\"Total no. of classes = \", self.c)\n",
        "    print(\"The class names are: \", self.classes)\n",
        "    print(\"Number of input features = \", self.inp_features)\n",
        "\n",
        "    # training samples size\n",
        "    self.m = self.x_train.shape[0]\n",
        "    print(\"Training samples = \", self.m)\n",
        "\n",
        "    # validation samples size\n",
        "    self.m_val = self.x_val.shape[0]\n",
        "    print(\"Validation samples = \", self.m_val)\n",
        "\n",
        "    # test samples size\n",
        "    self.m_test = self.x_test.shape[0]\n",
        "    print(\"Test samples = \", self.m_test)\n",
        "\n",
        "  def one_hot_encod(self):\n",
        "    self.y_train_encoded = np.eye(np.max(self.y_train) + 1)[self.y_train].T\n",
        "    self.y_val_encoded = np.eye(np.max(self.y_val) + 1)[self.y_val].T\n",
        "    self.y_test_encoded = np.eye(np.max(self.y_test) + 1)[self.y_test].T\n",
        "    return self.y_train_encoded, self.y_val_encoded, self.y_test_encoded\n",
        "  # Initializing parameters W and b\n",
        "  def initialize_parameters(self, nn_layers, weight_init=\"random\"):\n",
        "    '''\n",
        "    nn_layers: a list containing the number of neurons of each layer - where each layer no. is the index of the list\n",
        "    '''\n",
        "    np.random.seed(42)\n",
        "    parameters = {}                         # dictionary to hold weights and biases of each layer\n",
        "    u_t = {}\n",
        "    for i in range(1, len(nn_layers)):\n",
        "      if weight_init == \"xavier\":\n",
        "        parameters[\"W\"+str(i)] = np.random.randn(nn_layers[i],nn_layers[i-1])*np.sqrt(2/(nn_layers[i]+nn_layers[i-1]))\n",
        "        parameters[\"b\"+str(i)] = np.random.randn(nn_layers[i], 1)*np.sqrt(2/(nn_layers[i]))\n",
        "      elif weight_init == \"random\":\n",
        "        parameters[\"W\"+str(i)] = np.random.randn(nn_layers[i], nn_layers[i-1])\n",
        "        parameters[\"b\"+str(i)] = np.random.randn(nn_layers[i], 1)\n",
        "\n",
        "      u_t[\"W\"+str(i)] = np.zeros((nn_layers[i], nn_layers[i-1]))\n",
        "      u_t[\"b\"+str(i)] = np.zeros((nn_layers[i], 1))\n",
        "    return parameters, u_t\n",
        "\n",
        "  # Activation functions\n",
        "  def relu(self, x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def softmax(self, x):\n",
        "      return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "  # Derivatives of activation functions\n",
        "\n",
        "  def sigmoid_grad(self, x):\n",
        "      return self.sigmoid(x) * (1-self.sigmoid(x))\n",
        "\n",
        "  def relu_grad(self, x):\n",
        "      return 1*(x>0)\n",
        "\n",
        "  def tanh_grad(self, x):\n",
        "      return (1 - (np.tanh(x)**2))\n",
        "\n",
        "  def softmax_grad(self, x):\n",
        "      return self.softmax(x) * (1-self.softmax(x))\n",
        "\n",
        "\n",
        " # Forward propagation\n",
        "  def forward_propagation(self, x, nn_layers, parameters, act_fn):\n",
        "\n",
        "    l = len(nn_layers)  # No. of neural network layers, including input and output layers\n",
        "\n",
        "    a = {}              # dictionary to hold hidden layer (pre-activation)\n",
        "    h = {}              # dictionary to hold hidden layer (activation)\n",
        "\n",
        "    h[0] = x.T  # input layer\n",
        "\n",
        "    for i in range(1, l-1):\n",
        "      W = parameters[\"W\"+str(i)]        # weights of hidden layer i\n",
        "      b = parameters[\"b\"+str(i)]        # bias of hidden layer i\n",
        "\n",
        "      a[i] = np.matmul(W,h[i-1]) + b\n",
        "\n",
        "      # activation for hidden layers\n",
        "      if act_fn == 'sigmoid':\n",
        "        h[i] = self.sigmoid(a[i])\n",
        "      elif act_fn == 'relu':\n",
        "        h[i] = self.relu(a[i])\n",
        "      elif act_fn == 'tanh':\n",
        "        h[i] = self.tanh(a[i])\n",
        "\n",
        "    # output layer\n",
        "    W = parameters[\"W\"+str(l-1)]              # weights of hidden layer i\n",
        "    b = parameters[\"b\"+str(l-1)]              # bias of hidden layer i\n",
        "\n",
        "    a[l-1] = np.matmul(W,h[l-2]) + b          # activation function for output layer\n",
        "\n",
        "    y_hat = self.softmax(a[l-1])\n",
        "    return y_hat, h, a                # Returns y_hat, h, a\n",
        "\n",
        "  # Backpropagation\n",
        "  def back_propagation(self, y_hat, y, h, a, nn_layers, parameters, act_fn, batch_size, l2_reg, loss = 'cross_entropy'):\n",
        "    l = len(nn_layers)\n",
        "    grads = {}            # dictionary to store gradient of loss function wrt parameters and hidden layer neurons\n",
        "\n",
        "    # Computing gradient wrt output layer\n",
        "    if loss == 'cross_entropy':\n",
        "      grads[\"grada\"+str(l-1)] = y_hat - y\n",
        "    elif loss == 'mean_squared_error':\n",
        "      grads[\"grada\"+str(l-1)] = (y_hat - y)*self.softmax_grad(a[l-1])\n",
        "\n",
        "\n",
        "    for i in range(l-1,0,-1):\n",
        "      grads[\"gradW\" + str(i)] = (1/batch_size) * (np.dot(grads[\"grada\" + str(i)], h[i-1].T) + l2_reg*parameters[\"W\"+str(i)])\n",
        "      grads[\"gradb\" + str(i)] = (1/batch_size) * np.sum(grads[\"grada\" + str(i)], axis=1, keepdims=True)\n",
        "      if i>1:\n",
        "        if act_fn == 'sigmoid':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.sigmoid_grad(a[i-1])   # Computing gradients wrt hidden layers\n",
        "        elif act_fn == 'relu':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.relu_grad(a[i-1])\n",
        "        elif act_fn == 'tanh':\n",
        "          grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * self.tanh_grad(a[i-1])\n",
        "    return grads\n",
        "\n",
        "  # Update parameter based on different optimizers\n",
        "  def sgd(self, parameters, grads , lr):\n",
        "    l = self.hl + 1    # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)]\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)]\n",
        "    return parameters\n",
        "\n",
        "  def momentum(self, parameters, grads , lr, beta, u_t):\n",
        "    # u_t: accumulating history of the gradient of the parameters\n",
        "\n",
        "    l = self.hl + 1     # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "      u_t[\"W\"+str(i)] = beta*u_t[\"W\"+str(i)] + lr*grads[\"gradW\" + str(i)]\n",
        "      u_t[\"b\"+str(i)] = beta*u_t[\"b\"+str(i)] + lr*grads[\"gradb\" + str(i)]\n",
        "\n",
        "      parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - u_t[\"W\"+str(i)]\n",
        "      parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - u_t[\"b\"+str(i)]\n",
        "\n",
        "    return parameters, u_t\n",
        "\n",
        "  def nesterov(self, parameters, grads , lr, beta, u_t):\n",
        "    # u_t: accumulating history of the gradient of the parameters\n",
        "\n",
        "    l = self.hl + 1     # no. of hidden layers + outer layer\n",
        "    for i in range(1, l + 1):\n",
        "      u_t[\"W\"+str(i)] = beta*u_t[\"W\"+str(i)] + lr*grads[\"gradW\" + str(i)]\n",
        "      u_t[\"b\"+str(i)] = beta*u_t[\"b\"+str(i)] + lr*grads[\"gradb\" + str(i)]\n",
        "\n",
        "      parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - u_t[\"W\"+str(i)]\n",
        "      parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - u_t[\"b\"+str(i)]\n",
        "    return parameters, u_t\n",
        "\n",
        "  def rmsprop(self, parameters, grads, lr, beta, v_t):\n",
        "    # adapts learning rate of each parameter based on magnitude of recent gradients\n",
        "    # v_t: history of the gradient of parameters\n",
        "\n",
        "    l = self.hl + 1         # no. of hidden layers + outer layer\n",
        "    eps = 1e-4\n",
        "    for i in range(1, l + 1):\n",
        "\n",
        "        v_t[\"W\"+str(i)] = beta*v_t[\"W\" + str(i)] + (1-beta)*np.square(grads[\"gradW\" + str(i)])\n",
        "        v_t[\"b\"+str(i)] = beta*v_t[\"b\" + str(i)] + (1-beta)*np.square(grads[\"gradb\" + str(i)])\n",
        "\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)] / (np.sqrt(v_t[\"W\"+str(i)] + eps))\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)] / (np.sqrt(v_t[\"b\"+str(i)] + eps))\n",
        "    return parameters, v_t\n",
        "\n",
        "  def adam(self, parameters, grads, lr, m_w, v_w, step):\n",
        "    l = self.hl + 1       # number of layers in the neural network\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    eps = 1e-8\n",
        "\n",
        "    for i in range(1, l+1):\n",
        "        # computing intermediate values\n",
        "        m_w[\"W\"+str(i)] = beta1*m_w[\"W\"+str(i)] + (1-beta1)*grads[\"gradW\"+str(i)]\n",
        "        m_w[\"b\"+str(i)] = beta1*m_w[\"b\"+str(i)] + (1-beta1)*grads[\"gradb\"+str(i)]\n",
        "        v_w[\"W\"+str(i)] = beta2*v_w[\"W\"+str(i)] + (1-beta2)*np.square(grads[\"gradW\"+str(i)])\n",
        "        v_w[\"b\"+str(i)] = beta2*v_w[\"b\"+str(i)] + (1-beta2)*np.square(grads[\"gradb\"+str(i)])\n",
        "\n",
        "        m_w_hat = m_w[\"W\"+str(i)]/(1.0 - beta1**step)\n",
        "        m_b_hat = m_w[\"b\"+str(i)]/(1.0 - beta1**step)\n",
        "        v_w_hat = v_w[\"W\"+str(i)]/(1.0 - beta2**step)\n",
        "        v_b_hat = v_w[\"b\"+str(i)]/(1.0 - beta2**step)\n",
        "\n",
        "        #update parameters\n",
        "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - (lr * m_w_hat)/np.sqrt(v_w_hat + eps)\n",
        "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - (lr * m_b_hat)/np.sqrt(v_b_hat + eps)\n",
        "\n",
        "    step = step + 1\n",
        "    return parameters, m_w, v_w, step\n",
        "\n",
        "  def nadam(self, parameters, grads, lr, m_w, v_w, step):\n",
        "\n",
        "    l = self.hl + 1     # number of layers in the neural network\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    eps = 1e-8\n",
        "\n",
        "    for i in range(1, l+1):\n",
        "        # computing intermediate values\n",
        "        m_w[\"W\"+str(i)] = beta1*m_w[\"W\"+str(i)] + (1-beta1)*grads[\"gradW\"+str(i)]\n",
        "        m_w[\"b\"+str(i)] = beta1*m_w[\"b\"+str(i)] + (1-beta1)*grads[\"gradb\"+str(i)]\n",
        "        v_w[\"W\"+str(i)] = beta2*v_w[\"W\"+str(i)] + (1-beta2)*np.square(grads[\"gradW\"+str(i)])\n",
        "        v_w[\"b\"+str(i)] = beta2*v_w[\"b\"+str(i)] + (1-beta2)*np.square(grads[\"gradb\"+str(i)])\n",
        "\n",
        "        m_w_hat = m_w[\"W\"+str(i)]/(1.0 - beta1**step)\n",
        "        m_b_hat = m_w[\"b\"+str(i)]/(1.0 - beta1**step)\n",
        "        v_w_hat = v_w[\"W\"+str(i)]/(1.0 - beta2**step)\n",
        "        v_b_hat = v_w[\"b\"+str(i)]/(1.0 - beta2**step)\n",
        "\n",
        "        #update parameters\n",
        "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - (lr /np.sqrt(v_w_hat + eps))*(beta1*m_w_hat +\n",
        "                                                                                        (1-beta1)*grads[\"gradW\"+str(i)]/(1.0 - beta1**step))\n",
        "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - (lr /np.sqrt(v_b_hat + eps))*(beta1*m_b_hat +\n",
        "                                                                                        (1-beta1)*grads[\"gradb\"+str(i)]/(1.0 - beta1**step))\n",
        "\n",
        "    step = step + 1\n",
        "    return parameters, m_w, v_w, step\n",
        "\n",
        "  def compute_loss(self, y, y_hat, batch_size, parameters, l2_reg, loss = 'cross_entropy'):\n",
        "    # computing loss for cross entropy\n",
        "    wt_norm_sq = 0\n",
        "    for i in range(1, self.hl+2):\n",
        "        wt_norm_sq = wt_norm_sq + np.sum(np.square(parameters[\"W\"+str(i)]))\n",
        "    if loss == 'cross_entropy':\n",
        "        loss_val = (1/batch_size)*((-1)*np.sum(np.multiply(y, np.log(y_hat))) + 0.5*l2_reg*wt_norm_sq)\n",
        "    elif loss == 'mean_squared_error':\n",
        "        loss_val = (1/batch_size)*(np.sum((y-y_hat)**2) + 0.5*l2_reg*wt_norm_sq)\n",
        "    return loss_val\n",
        "\n",
        "\n",
        "  # Plotting function\n",
        "  def plot_loss_curve(self, training_loss, validation_loss):\n",
        "      \"\"\"\n",
        "      Plot the training and validation loss curves.\n",
        "      \"\"\"\n",
        "      n1 = len(training_loss)\n",
        "      n2 = len(validation_loss)\n",
        "      plt.plot(list(range(n1)), training_loss, 'b', label=\"Training Accuracy\")\n",
        "      plt.plot(list(range(n2)), validation_loss, 'r', label=\"Validation Accuracy\")\n",
        "\n",
        "      # Adding title and labels\n",
        "      plt.title(\"Training and Validation Accuracy vs Epochs\")\n",
        "      plt.xlabel(\"# Epochs\")\n",
        "      plt.ylabel(\"Accuracy\")\n",
        "\n",
        "      plt.grid()\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  def train_model(self, epochs = 10, num_hidden_layers = 3, num_neurons = 128, lr = 0.001, act_fn = 'relu',\n",
        "                    weight_init = 'xavier', optimizer = 'momentum',  batch_size = 512, l2_reg = 0.0005, loss = 'cross_entropy'):\n",
        "\n",
        "\n",
        "    self.hl = num_hidden_layers\n",
        "    self.nn = num_neurons\n",
        "    nn_layers = [self.inp_features] + [self.nn]*self.hl + [self.c]\n",
        "    parameters, u_t = self.initialize_parameters(nn_layers, weight_init)\n",
        "\n",
        "    params_lookahead = parameters.copy()                 # look ahead parameters init for nesterov/nadam\n",
        "\n",
        "    # adam/nadam optimizer utils\n",
        "    m_w = u_t.copy()\n",
        "    v_w = u_t.copy()\n",
        "    step = 1\n",
        "\n",
        "    l = num_hidden_layers + 1                                # no. of hidden layers + outer layer\n",
        "    beta = 0.9                                                # decay rate\n",
        "\n",
        "\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    training_acc = []\n",
        "    validation_acc = []\n",
        "    if optimizer == 'sgd':\n",
        "        batch_size = 1\n",
        "    for epoch in range(1, epochs+1):\n",
        "      for i in range(0, self.x_train.shape[0], batch_size):\n",
        "        batch_sz = min(batch_size, self.x_train.shape[0] - i)\n",
        "        y_hat, h, a = self.forward_propagation(self.x_train[i:i+batch_sz,:], nn_layers, parameters, act_fn)\n",
        "        if optimizer == 'nesterov' or optimizer == 'nadam':\n",
        "          # calculating look-ahead parameters\n",
        "          for j in range(1, l+1):\n",
        "            params_lookahead[\"W\"+str(j)] = parameters[\"W\"+str(j)] - beta*u_t[\"W\"+str(j)]\n",
        "            params_lookahead[\"b\"+str(j)] = parameters[\"b\"+str(j)] - beta*u_t[\"b\"+str(j)]\n",
        "          # calculating gradients of look-ahead\n",
        "          grads = self.back_propagation(y_hat, self.y_train_encoded[:,i:i+batch_sz], h, a, nn_layers, params_lookahead, act_fn, batch_sz, l2_reg, loss)\n",
        "          if optimizer == 'nesterov':\n",
        "            parameters, u_t = self.nesterov(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'nadam':\n",
        "            parameters, m_w, v_w, step = self.nadam(parameters, grads, lr, m_w, v_w, step)\n",
        "\n",
        "        else:\n",
        "          grads = self.back_propagation(y_hat, self.y_train_encoded[:,i:i+batch_sz], h, a, nn_layers, parameters, act_fn, batch_sz, l2_reg, loss)\n",
        "          if optimizer == 'sgd':\n",
        "            parameters = self.sgd(parameters, grads, lr)\n",
        "          elif optimizer == 'momentum':\n",
        "            parameters, u_t = self.momentum(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'rmsprop':\n",
        "            parameters, u_t = self.rmsprop(parameters, grads , lr, beta, u_t)\n",
        "          elif optimizer == 'adam':\n",
        "            parameters, m_w, v_w, step = self.adam(parameters, grads, lr, m_w, v_w, step)\n",
        "\n",
        "      # model evaluation\n",
        "      y_hat, _, _ = self.forward_propagation(self.x_train, nn_layers, parameters, act_fn)\n",
        "      cost = self.compute_loss(self.y_train_encoded, y_hat, y_hat.shape[1], parameters, l2_reg, loss)\n",
        "      training_loss.append(cost)\n",
        "\n",
        "      #one-hot encoding y_hat\n",
        "      y_hat_encoded = np.zeros_like(y_hat)\n",
        "      for i in range(y_hat.shape[1]):\n",
        "        max_index = np.argmax(y_hat[:, i])\n",
        "        y_hat_encoded[max_index, i] = 1\n",
        "      accuracy = np.mean(y_hat_encoded == self.y_train_encoded)\n",
        "      training_acc.append(accuracy)\n",
        "      # loss for the validation set\n",
        "      y_val_hat, _, _ = self.forward_propagation(self.x_val, nn_layers, parameters, act_fn)\n",
        "      val_cost = self.compute_loss(self.y_val_encoded, y_val_hat, y_val_hat.shape[1], parameters, l2_reg, loss)\n",
        "      validation_loss.append(val_cost)\n",
        "      #one-hot encoding y_val_hat\n",
        "      y_val_hat_encoded = np.zeros_like(y_val_hat)\n",
        "      for i in range(y_val_hat.shape[1]):\n",
        "        max_index = np.argmax(y_val_hat[:, i])\n",
        "        y_val_hat_encoded[max_index, i] = 1\n",
        "      val_accuracy = np.mean(y_val_hat_encoded == self.y_val_encoded)\n",
        "      validation_acc.append(val_accuracy)\n",
        "      #print(\"Epochs = \", epoch, \"\\tTraining cost:\", cost, \"\\tAccuracy:\", accuracy, \"\\t Validation Accuracy:\", val_accuracy)\n",
        "      # Export data to wandb\n",
        "      #wandb.log({ 'epoch': epoch, 'loss': cost, 'accuracy': accuracy * 100})\n",
        "      #wandb.log({ 'epoch': epoch, 'validation_loss': val_cost, 'validation_accuracy': val_accuracy * 100})\n",
        "    #print(f\"Training cost after {epochs} epochs:\", cost)\n",
        "    print(f\"Training accuracy after {epochs} epochs:\", accuracy)\n",
        "    print(f\"Validation accuracy after {epochs} epochs:\", val_accuracy)\n",
        "    #self.plot_loss_curve(training_acc, validation_acc)\n",
        "    return parameters, cost, accuracy\n",
        "\n",
        "  def test_model(self, parameters, num_hidden_layers, num_neurons, act_fn, l2_reg, loss):\n",
        "    self.hl = num_hidden_layers\n",
        "    self.nn = num_neurons\n",
        "    nn_layers = [self.inp_features] + [self.nn]*self.hl + [self.c]\n",
        "    y_hat, _, _ = self.forward_propagation(self.x_test, nn_layers, parameters, act_fn)\n",
        "    cost = self.compute_loss(self.y_test_encoded, y_hat, y_hat.shape[1], parameters, l2_reg, loss)\n",
        "    #one-hot encoding y_hat\n",
        "    y_hat_encoded = np.zeros_like(y_hat)\n",
        "    for i in range(y_hat.shape[1]):\n",
        "      max_index = np.argmax(y_hat[:, i])\n",
        "      y_hat_encoded[max_index, i] = 1\n",
        "    accuracy = np.mean(y_hat_encoded == self.y_test_encoded)\n",
        "    print(\"\\nTest Cost:\", cost, \"\\t Test Accuracy:\", accuracy)\n",
        "    #confused_mat = confusion_matrix(np.argmax(self.y_test_encoded, axis=1), np.argmax(y_hat_encoded, axis=1))\n",
        "    #print(\"Confusion Matrix:\")\n",
        "    #print(confused_mat)\n",
        "    #img  = plt.matshow(confused_mat)\n",
        "    #plt.title('Confusion matrix plot')\n",
        "    #plt.colorbar()\n",
        "    #plt.show()\n",
        "    return y_hat_encoded, cost, accuracy"
      ],
      "metadata": {
        "id": "F_Vclaf4VCTR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running model with best configuration 1"
      ],
      "metadata": {
        "id": "ZJmY9Tj2p1yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x, y), (x_test, y_test) = mnist.load_data()\n",
        "nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "nn.preprocess()\n",
        "nn.split_data()\n",
        "nn.print_data_details()\n",
        "_, _, y_test = nn.one_hot_encod()\n",
        "\n",
        "# Best hyperparameters after tuning with wandb\n",
        "epochs = 10\n",
        "num_hidden_layers = 3\n",
        "num_neurons = 128\n",
        "learning_rate = 1e-3\n",
        "act_fn = 'relu'\n",
        "optimizer = 'adam'\n",
        "batch_size = 128\n",
        "weight_init = 'xavier'\n",
        "l2_reg = 0.0005\n",
        "loss = 'cross_entropy'\n",
        "print(\"Training model with config 1\")\n",
        "parameters, cost, accuracy = nn.train_model(epochs, num_hidden_layers, num_neurons,\n",
        "                            learning_rate, act_fn, weight_init,\n",
        "                            optimizer, batch_size, l2_reg, loss)\n",
        "y_hat, _, _ = nn.test_model(parameters, num_hidden_layers, num_neurons, act_fn, l2_reg, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnSnortEhAbt",
        "outputId": "42b9f702-f997-40d9-f197-0a45f5a3152f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Total no. of classes =  10\n",
            "The class names are:  ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "Number of input features =  784\n",
            "Training samples =  54000\n",
            "Validation samples =  6000\n",
            "Test samples =  10000\n",
            "Training model with config 1\n",
            "Training accuracy after 10 epochs: 0.9986407407407407\n",
            "Validation accuracy after 10 epochs: 0.9956333333333334\n",
            "\n",
            "Test Cost: 0.10476789205935975 \t Test Accuracy: 0.99522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running model with configuration 2"
      ],
      "metadata": {
        "id": "1N5Fu8wg12Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x, y), (x_test, y_test) = mnist.load_data()\n",
        "nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "nn.preprocess()\n",
        "nn.split_data()\n",
        "nn.print_data_details()\n",
        "_, _, y_test = nn.one_hot_encod()\n",
        "\n",
        "# Best hyperparameters after tuning with wandb\n",
        "epochs = 10\n",
        "num_hidden_layers = 3\n",
        "num_neurons = 128\n",
        "learning_rate = 1e-3\n",
        "act_fn = 'relu'\n",
        "optimizer = 'nadam'\n",
        "batch_size = 128\n",
        "weight_init = 'xavier'\n",
        "l2_reg = 0.0005\n",
        "loss = 'cross_entropy'\n",
        "print(\"Training model with config 1\")\n",
        "parameters, cost, accuracy = nn.train_model(epochs, num_hidden_layers, num_neurons,\n",
        "                            learning_rate, act_fn, weight_init,\n",
        "                            optimizer, batch_size, l2_reg, loss)\n",
        "y_hat, _, _ = nn.test_model(parameters, num_hidden_layers, num_neurons, act_fn, l2_reg, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVltuS_Z106b",
        "outputId": "fe655009-89bc-4afc-d394-5c3288a8f98d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of classes =  10\n",
            "The class names are:  ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "Number of input features =  784\n",
            "Training samples =  54000\n",
            "Validation samples =  6000\n",
            "Test samples =  10000\n",
            "Training model with config 1\n",
            "Training accuracy after 10 epochs: 0.9985777777777778\n",
            "Validation accuracy after 10 epochs: 0.9954\n",
            "\n",
            "Test Cost: 0.1129693289872055 \t Test Accuracy: 0.99494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running model with configuration 3\n"
      ],
      "metadata": {
        "id": "tQjXtmTB1-d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x, y), (x_test, y_test) = mnist.load_data()\n",
        "nn = NeuralNetwork(x, y, x_test, y_test)\n",
        "nn.preprocess()\n",
        "nn.split_data()\n",
        "nn.print_data_details()\n",
        "_, _, y_test = nn.one_hot_encod()\n",
        "\n",
        "# Best hyperparameters after tuning with wandb\n",
        "epochs = 10\n",
        "num_hidden_layers = 3\n",
        "num_neurons = 128\n",
        "learning_rate = 1e-3\n",
        "act_fn = 'relu'\n",
        "optimizer = 'rmsprop'\n",
        "batch_size = 128\n",
        "weight_init = 'xavier'\n",
        "l2_reg = 0.0005\n",
        "loss = 'cross_entropy'\n",
        "print(\"Training model with config 1\")\n",
        "parameters, cost, accuracy = nn.train_model(epochs, num_hidden_layers, num_neurons,\n",
        "                            learning_rate, act_fn, weight_init,\n",
        "                            optimizer, batch_size, l2_reg, loss)\n",
        "y_hat, _, _ = nn.test_model(parameters, num_hidden_layers, num_neurons, act_fn, l2_reg, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Il7Gjm1z-y",
        "outputId": "994c285d-95bd-4074-fd42-2f3cce557419"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of classes =  10\n",
            "The class names are:  ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "Number of input features =  784\n",
            "Training samples =  54000\n",
            "Validation samples =  6000\n",
            "Test samples =  10000\n",
            "Training model with config 1\n",
            "Training accuracy after 10 epochs: 0.9975666666666667\n",
            "Validation accuracy after 10 epochs: 0.9951666666666666\n",
            "\n",
            "Test Cost: 0.08764979841492877 \t Test Accuracy: 0.99498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNgjbDbWhAIQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
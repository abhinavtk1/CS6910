{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MA23M002 - ABHINAV T K <br> CS6910 - Assignment 1"
      ],
      "metadata": {
        "id": "i_lW0takXFRr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ac3HqKeUWo8Z"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(x, y), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Change the shape of the data to (60000, 784)\n",
        "x = x.reshape(x.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "x = x/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "# Splitting the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "4GX1K-FDYhXu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset analysis and data preprocessing\n",
        "\n",
        "# no. of classes\n",
        "c = len(np.unique(y_train))\n",
        "print(\"Total no. of classes = \", c)\n",
        "\n",
        "# Input features\n",
        "inp_features = x_train.shape[1]\n",
        "print(\"Number of input features = \", inp_features)\n",
        "\n",
        "# training samples size\n",
        "m = x_train.shape[0]\n",
        "print(\"Training samples = \", m)\n",
        "\n",
        "# validation samples size\n",
        "m_val = x_val.shape[0]\n",
        "print(\"Validation samples = \", m_val)\n",
        "\n",
        "# test samples size\n",
        "m_test = x_test.shape[0]\n",
        "print(\"Test samples = \", m_test)\n",
        "\n",
        "# Class names - the index of the class names corresponds to the class label\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW-fm71NZps-",
        "outputId": "473c470c-5ae4-4326-a553-4917f246784f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of classes =  10\n",
            "Number of input features =  784\n",
            "Training samples =  54000\n",
            "Validation samples =  6000\n",
            "Test samples =  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding y\n",
        "y_train_encoded = np.eye(np.max(y_train) + 1)[y_train].T\n",
        "y_val_encoded = np.eye(np.max(y_val) + 1)[y_val].T\n",
        "y_test_encoded = np.eye(np.max(y_test) + 1)[y_test].T\n"
      ],
      "metadata": {
        "id": "M2glbIIYcDS3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Architecture"
      ],
      "metadata": {
        "id": "CBLEkfagcmXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining activation functions and their derivatives\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def softmax_grad(x):\n",
        "    return softmax(x) * (1-softmax(x))\n"
      ],
      "metadata": {
        "id": "VOh_ylS_clT2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters W and b\n",
        "\n",
        "def initialize_parameters(nn_layers):\n",
        "  '''\n",
        "  nn_layers: a list containing the number of neurons of each layer - where each layer no. is the index of the list\n",
        "  '''\n",
        "  np.random.seed(32)\n",
        "  parameters = {}                         # dictionary to hold weights and biases of each layer\n",
        "  for i in range(1, len(nn_layers)):\n",
        "    parameters[\"W\"+str(i)] = np.random.rand(nn_layers[i], nn_layers[i-1])*0.01\n",
        "    parameters[\"b\"+str(i)] = np.random.rand(nn_layers[i], 1)*0.01\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "3nv3ik7CclSr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3VO_xtCbTrn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(x, nn_layers, parameters):\n",
        "\n",
        "  l = len(nn_layers)  # 5 # No. of neural network layers, including input and output layers\n",
        "\n",
        "  a = {}              # dictionary to hold hidden layer (pre-activation)\n",
        "  h = {}              # dictionary to hold hidden layer (activation)\n",
        "\n",
        "  h[0] = x.T  # input layer\n",
        "  #print(h[0].shape)\n",
        "  for i in range(1, l-1):\n",
        "    W = parameters[\"W\"+str(i)]    # weights of hidden layer i\n",
        "    b = parameters[\"b\"+str(i)]    # bias of hidden layer i\n",
        "\n",
        "    a[i] = np.matmul(W,h[i-1]) + b\n",
        "\n",
        "    h[i] = sigmoid(a[i])          # activation for hidden layers\n",
        "\n",
        "  # output layer\n",
        "  W = parameters[\"W\"+str(l-1)]    # weights of hidden layer i\n",
        "  b = parameters[\"b\"+str(l-1)]    # bias of hidden layer i\n",
        "  a[l-1] = np.matmul(W,h[l-2]) + b          # activation function for output layer\n",
        "\n",
        "  y_hat = softmax(a[l-1])\n",
        "  return y_hat, h, a                # Returns y_hat, h, a"
      ],
      "metadata": {
        "id": "dAYJARfdmsYW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "def back_propagation(y_hat, y, h, a, nn_layers, parameters, batch_size):\n",
        "  l = len(nn_layers)\n",
        "  grads = {}            # dictionary to store gradient of loss function wrt parameters and hidden layer neurons\n",
        "\n",
        "  # Computing gradient wrt output layer\n",
        "  grads[\"grada\"+str(l-1)] = y_hat - y\n",
        "\n",
        "  for i in range(l-1,0,-1):\n",
        "    grads[\"gradW\" + str(i)] = (1/batch_size)*np.dot(grads[\"grada\" + str(i)], h[i-1].T)\n",
        "    grads[\"gradb\" + str(i)] = (1/batch_size)*np.sum(grads[\"grada\" + str(i)], axis=1, keepdims=True)\n",
        "    if i>1:\n",
        "      grads[\"grada\"+str(i-1)] = np.matmul(parameters[\"W\" + str(i)].T, grads[\"grada\" + str(i)]) * sigmoid_grad(a[i-1])   # Computing gradients wrt hidden layers\n",
        "  return grads"
      ],
      "metadata": {
        "id": "ddZEWVMOqRUG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads , lr):\n",
        "    l = len(parameters) // 2\n",
        "    for i in range(1, l + 1):\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - lr * grads[\"gradW\" + str(i)]\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - lr * grads[\"gradb\" + str(i)]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "yXohNXrBt7qM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y, y_hat, batch_size, parameters):\n",
        "  loss = (1/batch_size)*(-1.0 * np.sum(np.multiply(y, np.log(y_hat))))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "vfSpTZrB2nYq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training model\n",
        "hidden_layers = [128, 64, 32]\n",
        "nn_layers = [inp_features] + hidden_layers + [c]\n",
        "parameters = initialize_parameters(nn_layers)\n",
        "iter = 0\n",
        "epochs = 10\n",
        "lr = 0.01 # learning rate\n",
        "batch_size = 64\n",
        "\n",
        "while iter < epochs:\n",
        "  iter += 1\n",
        "  for i in range(0, x_train.shape[0], batch_size):\n",
        "    batch_count = batch_size\n",
        "    if i + batch_size > x_train.shape[0]: # the last mini-batch might contain fewer than \"batch_size\" examples\n",
        "      batch_count = x_train.shape[0] - i + 1\n",
        "\n",
        "    y_hat, h, a = forward_propagation(x_train[i:i+batch_size,:], nn_layers, parameters)\n",
        "    grads = back_propagation(y_hat, y_train_encoded[:,i:i+batch_size], h, a, nn_layers, parameters, batch_size)\n",
        "    parameters = update_parameters(parameters, grads, lr)\n",
        "\n",
        "  # Mean loss for the full training set\n",
        "  y_hat, _, _ = forward_propagation(x_train, nn_layers, parameters)\n",
        "  cost = compute_loss(y_train_encoded, y_hat, 54000, parameters)\n",
        "  y_hat = np.argmax(y_hat, axis=0)\n",
        "  accuracy = np.mean(y_hat == y_train_encoded)\n",
        "  print(\"Epochs = \", iter, \"\\tTraining cost:\", cost, \"\\tAccuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXPaZjpEv8cl",
        "outputId": "3580a494-b9b9-41b9-f7b5-7e636fd20133"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs =  1 \tTraining cost: 2.302679361987962 \tAccuracy: 0.1\n",
            "Epochs =  2 \tTraining cost: 2.3026778773319814 \tAccuracy: 0.1\n",
            "Epochs =  3 \tTraining cost: 2.3026764068354377 \tAccuracy: 0.1\n",
            "Epochs =  4 \tTraining cost: 2.302674973154227 \tAccuracy: 0.1\n",
            "Epochs =  5 \tTraining cost: 2.302673574290012 \tAccuracy: 0.1\n",
            "Epochs =  6 \tTraining cost: 2.302672208268539 \tAccuracy: 0.1\n",
            "Epochs =  7 \tTraining cost: 2.302670873135556 \tAccuracy: 0.1\n",
            "Epochs =  8 \tTraining cost: 2.302669566949644 \tAccuracy: 0.1\n",
            "Epochs =  9 \tTraining cost: 2.302668287774903 \tAccuracy: 0.1\n",
            "Epochs =  10 \tTraining cost: 2.302667033673437 \tAccuracy: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxNYXccvQm_6"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}